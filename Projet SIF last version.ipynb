{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14574cd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913287ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from folders\n",
    "def load_images_from_folder(path):\n",
    "    images = []\n",
    "    for folder in path:\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(img_path):\n",
    "                image = io.imread(img_path, as_gray=True)\n",
    "                images.append(image.flatten())\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e615fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "angry_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\angry'  # Path to the folder containing angry images\n",
    "disgust_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\disgust'  # Path to the folder containing disgust images\n",
    "fear_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\fear'  # Path to the folder containing fear images\n",
    "happy_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\happy'  # Path to the folder containing happy images\n",
    "neutral_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\neutral'  # Path to the folder containing neutral images\n",
    "sad_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\sad'  # Path to the folder containing sad images\n",
    "surprise_folder = r'C:\\Users\\hp\\Desktop\\archive\\train\\surprise'  # Path to the folder containing surprise images\n",
    "\n",
    "train_data_angry = load_images_from_folder([angry_folder])\n",
    "train_data_disgust = load_images_from_folder([disgust_folder])\n",
    "train_data_fear = load_images_from_folder([fear_folder])\n",
    "train_data_happy = load_images_from_folder([happy_folder])\n",
    "train_data_neutral = load_images_from_folder([neutral_folder])\n",
    "train_data_sad = load_images_from_folder([sad_folder])\n",
    "train_data_surprise = load_images_from_folder([surprise_folder])\n",
    "\n",
    "# Combine and label the training data\n",
    "train_data = np.vstack((train_data_angry, train_data_disgust, train_data_fear,\n",
    "                        train_data_happy, train_data_neutral, train_data_sad, train_data_surprise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e22c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 7\n",
    "emotions=['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/2000], Loss: 1.8211\n",
      "Epoch [20/2000], Loss: 1.8211\n",
      "Epoch [30/2000], Loss: 1.8211\n",
      "Epoch [40/2000], Loss: 1.8211\n",
      "Epoch [50/2000], Loss: 1.8211\n",
      "Epoch [60/2000], Loss: 1.8211\n",
      "Epoch [70/2000], Loss: 1.8211\n",
      "Epoch [80/2000], Loss: 1.8211\n",
      "Epoch [90/2000], Loss: 1.8211\n",
      "Epoch [100/2000], Loss: 1.8211\n",
      "Epoch [110/2000], Loss: 1.8211\n",
      "Epoch [120/2000], Loss: 1.8211\n",
      "Epoch [130/2000], Loss: 1.8211\n",
      "Epoch [140/2000], Loss: 1.8211\n",
      "Epoch [150/2000], Loss: 1.8211\n",
      "Epoch [160/2000], Loss: 1.8211\n",
      "Epoch [170/2000], Loss: 1.8211\n",
      "Epoch [180/2000], Loss: 1.8211\n",
      "Epoch [190/2000], Loss: 1.8211\n",
      "Epoch [200/2000], Loss: 1.8211\n",
      "Epoch [210/2000], Loss: 1.8211\n",
      "Epoch [220/2000], Loss: 1.8211\n",
      "Epoch [230/2000], Loss: 1.8211\n",
      "Epoch [240/2000], Loss: 1.8211\n",
      "Epoch [250/2000], Loss: 1.8211\n",
      "Epoch [260/2000], Loss: 1.8211\n",
      "Epoch [270/2000], Loss: 1.8211\n",
      "Epoch [280/2000], Loss: 1.8211\n",
      "Epoch [290/2000], Loss: 1.8211\n",
      "Epoch [300/2000], Loss: 1.8211\n",
      "Epoch [310/2000], Loss: 1.8211\n",
      "Epoch [320/2000], Loss: 1.8211\n",
      "Epoch [330/2000], Loss: 1.8211\n",
      "Epoch [340/2000], Loss: 1.8211\n",
      "Epoch [350/2000], Loss: 1.8211\n",
      "Epoch [360/2000], Loss: 1.8211\n",
      "Epoch [370/2000], Loss: 1.8211\n",
      "Epoch [380/2000], Loss: 1.8211\n",
      "Epoch [390/2000], Loss: 1.8211\n",
      "Epoch [400/2000], Loss: 1.8211\n",
      "Epoch [410/2000], Loss: 1.8211\n",
      "Epoch [420/2000], Loss: 1.8211\n",
      "Epoch [430/2000], Loss: 1.8211\n",
      "Epoch [440/2000], Loss: 1.8211\n",
      "Epoch [450/2000], Loss: 1.8211\n",
      "Epoch [460/2000], Loss: 1.8211\n",
      "Epoch [470/2000], Loss: 1.8211\n",
      "Epoch [480/2000], Loss: 1.8211\n",
      "Epoch [490/2000], Loss: 1.8211\n",
      "Epoch [500/2000], Loss: 1.8211\n",
      "Epoch [510/2000], Loss: 1.8211\n",
      "Epoch [520/2000], Loss: 1.8211\n",
      "Epoch [530/2000], Loss: 1.8211\n",
      "Epoch [540/2000], Loss: 1.8211\n",
      "Epoch [550/2000], Loss: 1.8211\n",
      "Epoch [560/2000], Loss: 1.8211\n",
      "Epoch [570/2000], Loss: 1.8211\n",
      "Epoch [580/2000], Loss: 1.8211\n",
      "Epoch [590/2000], Loss: 1.8211\n",
      "Epoch [600/2000], Loss: 1.8211\n",
      "Epoch [610/2000], Loss: 1.8211\n",
      "Epoch [620/2000], Loss: 1.8211\n",
      "Epoch [630/2000], Loss: 1.8211\n",
      "Epoch [640/2000], Loss: 1.8211\n",
      "Epoch [650/2000], Loss: 1.8211\n",
      "Epoch [660/2000], Loss: 1.8211\n",
      "Epoch [670/2000], Loss: 1.8211\n",
      "Epoch [680/2000], Loss: 1.8211\n",
      "Epoch [690/2000], Loss: 1.8211\n",
      "Epoch [700/2000], Loss: 1.8211\n",
      "Epoch [710/2000], Loss: 1.8211\n"
     ]
    }
   ],
   "source": [
    "m = 2  # Fuzziness coefficient\n",
    "tolerance = 1e-6  # Convergence threshold\n",
    "num_epochs = 2000\n",
    "dropout_rate = 0.2  # Dropout rate for regularization\n",
    "\n",
    "# Initialize the membership matrix\n",
    "num_samples = len(train_data)\n",
    "membership = np.random.rand(num_samples, num_clusters)\n",
    "membership = membership / np.sum(membership, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Fuzzy C-means algorithm\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the cluster centers\n",
    "    cluster_centers = np.dot(membership.T, train_data) / np.sum(membership, axis=0)[:, np.newaxis]\n",
    "\n",
    "    # Compute the distances between samples and cluster centers\n",
    "    distances = np.linalg.norm(train_data[:, np.newaxis] - cluster_centers, axis=2)\n",
    "\n",
    "    # Update the membership matrix\n",
    "    new_membership = 1 / (distances ** (2 / (m - 1)))\n",
    "    new_membership = new_membership / np.sum(new_membership, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Check for convergence\n",
    "    if np.max(np.abs(new_membership - membership)) < tolerance:\n",
    "        break\n",
    "\n",
    "    membership = new_membership\n",
    "\n",
    "# Assign labels based on the highest membership value\n",
    "predicted_labels = np.argmax(membership, axis=1)\n",
    "\n",
    "# Convert the labels to TensorFlow tensor\n",
    "train_labels = tf.convert_to_tensor(predicted_labels, dtype=tf.int64)\n",
    "\n",
    "# Define the LVQ model\n",
    "class LVQ(keras.Model):\n",
    "    def __init__(self, num_classes, input_dim, prototypes_per_class, dropout_rate):\n",
    "        super(LVQ, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        self.prototypes_per_class = prototypes_per_class\n",
    "\n",
    "        self.prototypes = self.add_weight(shape=(num_classes * prototypes_per_class, input_dim),\n",
    "                                          initializer='random_normal',\n",
    "                                          trainable=False)  # Freeze the prototypes during training\n",
    "        self.dropout = keras.layers.Dropout(dropout_rate)  # Add dropout layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # Compute distances between input samples and prototypes\n",
    "        distances = tf.norm(x[:, tf.newaxis] - self.prototypes, axis=-1)\n",
    "\n",
    "        # Get the nearest prototypes for each input sample\n",
    "        nearest_prototypes = tf.argmin(distances, axis=-1)\n",
    "\n",
    "        return nearest_prototypes\n",
    "\n",
    "\n",
    "# Convert the data to TensorFlow tensors\n",
    "train_data = tf.convert_to_tensor(train_data, dtype=tf.float32)\n",
    "\n",
    "# Create an instance of the LVQ model\n",
    "#model = LVQ(num_clusters, train_data.shape[1], 1)\n",
    "model = LVQ(num_clusters, train_data.shape[1], 1, dropout_rate)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        outputs = model(train_data)\n",
    "        # Convert the outputs to one-hot encoded logits\n",
    "        logits = tf.one_hot(outputs, depth=num_clusters)\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(train_labels, logits)\n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Print the loss for monitoring\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a new image\n",
    "new_image_path = \"C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\test\\\\sad\\\\PrivateTest_366361.jpg\"  # Path to the new image file\n",
    "if os.path.isfile(new_image_path):\n",
    "    new_image = io.imread(new_image_path, as_gray=True)\n",
    "    new_image = np.array(new_image.flatten())\n",
    "    new_image = tf.convert_to_tensor(new_image, dtype=tf.float32)\n",
    "    new_image = tf.expand_dims(new_image, axis=0)  # Add a batch dimension\n",
    "\n",
    "    predicted_label = model(new_image)\n",
    "    predicted_class = emotions[predicted_label.numpy().item()]  \n",
    "\n",
    "    print(\"Predicted emotion:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b106564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a new image\n",
    "new_image_path = \"C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\test\\\\surprise\\\\PrivateTest_2017514.jpg\"  # Path to the new image file\n",
    "if os.path.isfile(new_image_path):\n",
    "    new_image = io.imread(new_image_path, as_gray=True)\n",
    "    new_image = np.array(new_image.flatten())\n",
    "    new_image = tf.convert_to_tensor(new_image, dtype=tf.float32)\n",
    "    new_image = tf.expand_dims(new_image, axis=0)  # Add a batch dimension\n",
    "\n",
    "    predicted_label = model(new_image)\n",
    "    predicted_class = emotions[predicted_label.numpy().item()]\n",
    "\n",
    "    print(\"Predicted emotion:\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04811a",
   "metadata": {},
   "source": [
    "# add normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60544b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to load images from folders\n",
    "def load_images_from_folder(folders):\n",
    "    images = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(img_path):\n",
    "                image = io.imread(img_path, as_gray=True)\n",
    "                images.append(image.flatten())\n",
    "    return np.array(images)\n",
    "\n",
    "# Normalize the input data\n",
    "def normalize_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_normalized = scaler.fit_transform(data)\n",
    "    return data_normalized\n",
    "\n",
    "# Training data\n",
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "train_data_folders = [f'C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\train\\\\{emotion}' for emotion in emotions]\n",
    "train_data = load_images_from_folder(train_data_folders)\n",
    "\n",
    "# Normalize the training data\n",
    "train_data = normalize_data(train_data)\n",
    "\n",
    "# Labels for training data\n",
    "train_labels = np.concatenate([np.full(len(data), i) for i, data in enumerate(train_data_folders)])\n",
    "\n",
    "# Convert the labels to TensorFlow tensor\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int64)\n",
    "\n",
    "# Shuffle the data and labels\n",
    "perm = np.random.permutation(len(train_labels))\n",
    "train_data = train_data[perm]\n",
    "train_labels = tf.gather(train_labels, perm)\n",
    "\n",
    "# Convert train_labels to NumPy array\n",
    "train_labels = train_labels.numpy()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "num_clusters = len(emotions)\n",
    "m = 2  # Fuzziness coefficient\n",
    "tolerance = 1e-6  # Convergence threshold\n",
    "num_epochs = 10000\n",
    "dropout_rate = 0.2  # Dropout rate for regularization\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the membership matrix\n",
    "num_samples = len(train_data)\n",
    "membership = np.random.rand(num_samples, num_clusters)\n",
    "membership = membership / np.sum(membership, axis=1)[:, np.newaxis]\n",
    "\n",
    "# Fuzzy C-means algorithm\n",
    "for epoch in range(num_epochs):\n",
    "    # Compute the cluster centers\n",
    "    cluster_centers = np.dot(membership.T, train_data) / np.sum(membership, axis=0)[:, np.newaxis]\n",
    "\n",
    "    # Compute the distances between samples and cluster centers\n",
    "    distances = np.linalg.norm(train_data[:, np.newaxis] - cluster_centers, axis=2)\n",
    "\n",
    "    # Update the membership matrix\n",
    "    new_membership = 1 / (distances ** (2 / (m - 1)))\n",
    "    new_membership = new_membership / np.sum(new_membership, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Check for convergence\n",
    "    if np.max(np.abs(new_membership - membership)) < tolerance:\n",
    "        break\n",
    "\n",
    "    membership = new_membership\n",
    "\n",
    "# Assign labels based on the highest membership value\n",
    "predicted_labels = np.argmax(membership, axis=1)\n",
    "\n",
    "# Convert the labels to TensorFlow tensor\n",
    "predicted_labels = tf.convert_to_tensor(predicted_labels, dtype=tf.int64)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = np.mean(predicted_labels == train_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94671e0",
   "metadata": {},
   "source": [
    "# add metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy_train_path=\"C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\train\\\\happy\"\n",
    "sad_train_path=\"C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\train\\\\sad\"\n",
    "surprised_train_path = \"C:\\\\Users\\\\hp\\\\Desktop\\\\archive\\\\train\\\\surprise\"\n",
    "\n",
    "# Function to load images from folders\n",
    "def load_images_from_folder(path):\n",
    "    images = []\n",
    "    for folder in path:\n",
    "        for filename in os.listdir(folder):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            if os.path.isfile(img_path):\n",
    "                image = io.imread(img_path, as_gray=True)\n",
    "                images.append(image.flatten() / 255.)\n",
    "    return np.array(images)\n",
    "\n",
    "# Training data\n",
    "train_data_happy = load_images_from_folder([happy_train_path])\n",
    "train_data_sad = load_images_from_folder([sad_train_path])\n",
    "train_data_surprised = load_images_from_folder([surprised_train_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cee669",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.vstack((train_data_happy, train_data_sad, train_data_surprised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = [0 for e in range(train_data_happy.shape[0])]\n",
    "sa = [1 for e in range(train_data_sad.shape[0])]\n",
    "su = [2 for e in range(train_data_surprised.shape[0])]\n",
    "train_labels = ha + sa + su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = train_labels\n",
    "\n",
    "import random as rd\n",
    "class FCM:\n",
    "    def __init__(self, m, c, X):\n",
    "        self.m = m\n",
    "        self.c = c\n",
    "        self.p = X.shape[1]\n",
    "        minX = [min(X[:,i]) for i in range(X.shape[1])]\n",
    "        maxX = [max(X[:,i]) for i in range(X.shape[1])]\n",
    "        self.V = np.array([ [ rd.uniform(minX[j], maxX[j]) for j in range(X.shape[1])] for i in range(self.c)])\n",
    "        self.U = []\n",
    "        self.Etqt = []\n",
    "    def calculerU(self, X):\n",
    "        alpha = -1./(self.m-1.)\n",
    "        term1 = [ [ pow(sum(np.square(X[k]-self.V[i])), alpha) for k in range(X.shape[0]) ] for i in range(self.c)]\n",
    "        term2 = [ sum([pow(sum(np.square(X[k]-self.V[i])), alpha) for i in range(self.c)]) for k in range(X.shape[0])]\n",
    "        self.U = [ [ term1[i][k]/term2[k] for k in range(X.shape[0])] for i in range(self.c)]\n",
    "    def calculerV(self, X):\n",
    "        card = [ sum([pow(self.U[i][k], self.m) for k in range(X.shape[0])]) for i in range(self.c)]\n",
    "        #print(sum(card)) #à cause de la puissance m, la somme est différente de n\n",
    "        term = [ [ sum([ pow(self.U[i][k], self.m)*X[k][j] for k in range(X.shape[0])]) for j in range(X.shape[1])] for i in range(self.c)]\n",
    "        self.V = [ [ term[i][j]/card[i] for j in range(X.shape[1])] for i in range(self.c)]\n",
    "    def entrainer(self, X, tmax, eps):\n",
    "        t = 1\n",
    "        while(t<tmax):\n",
    "            print(t)\n",
    "            self.calculerU(X)\n",
    "            self.calculerV(X)\n",
    "            t = t + 1\n",
    "            self.etiqueterV(X)\n",
    "            #print(self.tauxErr(X, D)*100./X.shape[0],\"%\")\n",
    "    def etiqueterV(self, X):\n",
    "        card = [0 for i in range(self.c)]\n",
    "        Vmoy = [ [ 0. for j in range(X.shape[1])] for i in range(self.c)]\n",
    "        for k in range(X.shape[0]):\n",
    "            for j in range(X.shape[1]):\n",
    "                Vmoy[D[k]-1][j] = Vmoy[D[k]-1][j] + X[k][j]\n",
    "        for k in range(X.shape[0]):\n",
    "            card[D[k]-1] = card[D[k]-1] + 1\n",
    "        for i in range(self.c):\n",
    "            for j in range(X.shape[1]):\n",
    "                Vmoy[i][j] = Vmoy[i][j] / card[D[k]-1]\n",
    "        Vmoy = np.array(Vmoy)\n",
    "        #print(\"Vmoy = \\n\"+str(np.array(Vmoy)))\n",
    "        dist = [[sum(np.square(self.V[l]-Vmoy[i])) for i in range(self.c)] for l in range(self.c)]\n",
    "        etqt = [np.argmin(dist[i]) for i in range(self.c)]\n",
    "        for i in range(self.c):\n",
    "            self.Etqt.append(etqt[i]+1)\n",
    "        #print(np.array(self.Etqt))i\n",
    "    def classe(self, vect):\n",
    "        term1 = [ pow(sum(np.square(vect-self.V[i])), -1./(self.m-1)) for i in range(self.c)]\n",
    "        term2 = sum([ pow(sum(np.square(vect-self.V[i])), -1./(self.m-1)) for i in range(self.c)])\n",
    "        u = [term1[i]/term2 for i in range(self.c)]\n",
    "        return self.Etqt[np.argmax(u)]\n",
    "\n",
    "    def tauxErr(self, X, D):\n",
    "        er = 0.\n",
    "        for k in range(X.shape[0]):\n",
    "            if(self.classe(X[k]) != D[k]):\n",
    "                er = er + 1\n",
    "        return er\n",
    "    \n",
    "    def entropie(self,X):\n",
    "        En = []\n",
    "        En = sum(sum(  self.U[i][k]*(math.log2(self.U[i][k]) ) for i in range(self.c)) for k in range(X.shape[0] ))\n",
    "        return -En/X.shape[0]\n",
    "    \n",
    "    def pc(self, X):\n",
    "        xp = sum(sum(np.square(fcm.U[i][k]) for i in range(c)) for k in range(X.shape[0]))\n",
    "        return xp/X.shape[0]\n",
    "\n",
    "    def matriceConfusion(self,X, D):\n",
    "        matConf = np.zeros((fcm.c, fcm.c), dtype=int)\n",
    "        for i in range(X.shape[0]):\n",
    "            vraieClasse = D[i]-1\n",
    "            predClasse = fcm.classe(X[i])-1\n",
    "            matConf[vraieClasse, predClasse] += 1\n",
    "        return matConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fcm = FCM(2, 3, train_data)\n",
    "fcm.calculerU(train_data)\n",
    "#for i in range(X.shape[0]):\n",
    "#print(np.array(fcm.U)[:,i])\n",
    "fcm.calculerV(train_data)\n",
    "print(\"les prototypes avant l'entrainement :\\n\",np.array(fcm.V))\n",
    "print()\n",
    "fcm.entrainer(train_data, 1000, 0.001)\n",
    "print(\"les prototypes apres l'entrainement :\\n\",np.array(fcm.V))\n",
    "# Créer la matrice de confusion en utilisant scikit-learn\n",
    "confusion = confusion_matrix(train_labels, fcm.predict(train_data))\n",
    "print(\"la matrice de confusion est :\\n\",confusion)\n",
    "#print(fcm.tauxErr(X, D)*100./X.shape[0],\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92498b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
